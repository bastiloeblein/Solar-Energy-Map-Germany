{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e874ea7-3f7b-4b80-afad-2a674413648d",
   "metadata": {},
   "source": [
    "## Markstammdatenregister: Data Preparation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e2fda2c-6270-4f88-bfb2-5ba5371a6703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import json\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14cde1ec-38e9-40f3-b14d-337417549f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to import and their data types\n",
    "SELECTED_COLS = [\n",
    "    'MaStR-Nr. der Einheit',\n",
    "    'Betriebs-Status',\n",
    "    'Energieträger',\n",
    "    'Bruttoleistung der Einheit',\n",
    "    'Nettonennleistung der Einheit',\n",
    "    'Bundesland',\n",
    "    'Postleitzahl',\n",
    "    'Straße',\n",
    "    'Hausnummer',\n",
    "    'Koordinate: Breitengrad (WGS84)',\n",
    "    'Koordinate: Längengrad (WGS84)',\n",
    "    'Inbetriebnahmedatum der Einheit',\n",
    "    'Anzahl der Solar-Module',\n",
    "    'Hauptausrichtung der Solar-Module',\n",
    "    'Lage der Einheit',\n",
    "    'Name des Anlagenbetreibers (nur Org.)'\n",
    "]\n",
    "\n",
    "dtype_dict = {\n",
    "    'MaStR-Nr. der Einheit': str,\n",
    "    'Betriebs-Status': str,\n",
    "    'Energieträger': str,\n",
    "    'Bruttoleistung der Einheit': float,\n",
    "    'Nettonennleistung der Einheit': float,\n",
    "    'Bundesland': str,\n",
    "    'Postleitzahl': 'Int64',  \n",
    "    'Straße': str,\n",
    "    'Hausnummer': str,\n",
    "    'Koordinate: Breitengrad (WGS84)': float,\n",
    "    'Koordinate: Längengrad (WGS84)': float,\n",
    "    'Inbetriebnahmedatum der Einheit': str,\n",
    "    'Anzahl der Solar-Module': 'Int64',\n",
    "    'Hauptausrichtung der Solar-Module': str,\n",
    "    'Lage der Einheit': str,\n",
    "    'Name des Anlagenbetreibers (nur Org.)': str\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ad4c1-b43c-4d7d-993a-4361e46005fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zum Verzeichnis\n",
    "input_directory = 'data_raw/'\n",
    "output_file = 'data_raw_merged.csv'\n",
    "\n",
    "# Liste alle CSV-Dateien im Verzeichnis auf\n",
    "csv_files = sorted([file for file in os.listdir(input_directory) if file.endswith('.csv')])\n",
    "\n",
    "# Initialisiere eine Liste, um alle DataFrames zu speichern\n",
    "dataframes_list = []\n",
    "\n",
    "# Durchlaufe alle CSV-Dateien und verarbeite jede\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(input_directory, csv_file)\n",
    "    try:\n",
    "        # Lese die CSV-Datei\n",
    "        data = pd.read_csv(file_path, usecols=SELECTED_COLS, dtype=dtype_dict, delimiter=';', decimal=',', on_bad_lines='skip')\n",
    "        print(data.shape)\n",
    "\n",
    "        # Füge den DataFrame zur Liste hinzu\n",
    "        dataframes_list.append(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Lesen von {csv_file}: {e}\")\n",
    "\n",
    "# Verbinde alle DataFrames in der Liste zu einem einzigen DataFrame\n",
    "df_original = pd.concat(dataframes_list, ignore_index=True)\n",
    "\n",
    "# Ausgabe der Gesamtanzahl der Zeilen des zusammengeführten DataFrames\n",
    "print(f\"Gesamtanzahl der Zeilen nach dem Zusammenführen: {len(df_original)}\")\n",
    "\n",
    "df_original.to_csv(output_file, index=False, sep = \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f89484d-9846-4024-af4b-61065b64271f",
   "metadata": {},
   "source": [
    "### Explorative Daten Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a4f0bb-b9f2-4136-8120-901be080dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechne die Häufigkeit der Energieträger\n",
    "value_counts = df_original['Energieträger'].value_counts()\n",
    "\n",
    "# Berechne die prozentualen Anteile\n",
    "percentage = (value_counts / value_counts.sum()) * 100\n",
    "\n",
    "# Selektiere die Top 10 Energieträger\n",
    "top_10 = percentage.head(10)\n",
    "\n",
    "print(top_10)\n",
    "\n",
    "# Erstelle das Säulendiagramm\n",
    "plt.figure(figsize=(20, 10))\n",
    "ax = top_10.plot(kind='bar', color='skyblue')\n",
    "plt.title('Top 10 Energieträger im Marktstammdatenregister')\n",
    "plt.xlabel('Energieträger')\n",
    "plt.ylabel('Prozent')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(range(0, 101, 10), [f'{i}%' for i in range(0, 101, 10)])  # Setzt die Y-Achse in 10% Schritten\n",
    "\n",
    "# Füge die absoluten Zahlen über die Balken\n",
    "for index, bar in enumerate(ax.patches):\n",
    "    height = bar.get_height()\n",
    "    label = f'{value_counts[top_10.index[index]]:,}'  # Formatierung mit Tausendertrennzeichen\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, height + 0.5, label, ha='center')\n",
    "\n",
    "# Save figure for Paper\n",
    "plt.savefig(\"Verteilung_Energieträger.png\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b7eb9-cb1b-47be-8eef-5691b5f43e04",
   "metadata": {},
   "source": [
    "### Data Preparation and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c2206-a23e-400b-b811-1aa99d66378d",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96fab47-c32b-43a5-81de-bb0c62198d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Löschen von NaNs relevanter Spalten\n",
    "before = df_original.shape[0]\n",
    "filtered_data = df_original.dropna(subset=[\"Inbetriebnahmedatum der Einheit\", \"Postleitzahl\", \"Name des Anlagenbetreibers (nur Org.)\", \"Bundesland\"])\n",
    "print(\"Gelöschte Einträge: \", before - filtered_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207aedd3-8f57-4bf2-9246-9c371cf86caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtern nach relevanten Einträgen\n",
    "before = filtered_data.shape[0]\n",
    "filtered_data = filtered_data[\n",
    "        (filtered_data['Betriebs-Status'].isin(['In Betrieb', 'In Planung'])) &\n",
    "        (filtered_data['Energieträger'] == 'Solare Strahlungsenergie')\n",
    "    ]\n",
    "print(\"Gelöschte Einträge: \", before - filtered_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada9a7d4-8e02-4f28-b9f7-2010ca484b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Data\n",
    "print(f\"Shape Originaldatensatz: {df_original.shape}\")\n",
    "print(f\"Shape gefilterter Datensatz: {filtered_data.shape}\")\n",
    "print(f\"Anzahl gelöschte Zeilen: {df_original.shape[0] - filtered_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a67b8ab-d64d-4b0e-be9c-477f834ff67b",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a4fe325-b6c0-434b-93ba-02a43f7d7978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neue Spalte Betreiber erstellen\n",
    "df = filtered_data\n",
    "df[\"Betreiber\"] = df['Name des Anlagenbetreibers (nur Org.)'].apply(\n",
    "    lambda x: \"natürliche Person\" if isinstance(x, str) and \"natürliche Person\" in x else x)\n",
    "df['Betreiber'] = df['Betreiber'].replace({\n",
    "    'Lidl Vertriebs GmbH & Co. KG': 'Lidl Vertriebs-GmbH & Co. KG'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6efea098-62e3-4ac7-a2b0-c607eeb372e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konvertieren von Inbetriebnahmedatum zu Datum und Extraktion des Jahres\n",
    "df['Inbetriebnahmedatum der Einheit formatted'] = pd.to_datetime(df['Inbetriebnahmedatum der Einheit'], format= \"%d/%m/%Y\")\n",
    "df['Year'] = df['Inbetriebnahmedatum der Einheit formatted'].dt.year.astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b24761e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add leading zero if PLZ only has 4 chars\n",
    "df[\"Postleitzahl\"] = df[\"Postleitzahl\"].astype(float).astype(int)\n",
    "df[\"Postleitzahl\"] = df[\"Postleitzahl\"].astype(str)\n",
    "df[\"Postleitzahl\"] = df[\"Postleitzahl\"].apply(lambda x: x.zfill(5) if len(x) == 4 else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615875cf-bcec-4d0d-b6a1-d05ec07b7528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if successful\n",
    "test = df[\"Postleitzahl\"].to_list()\n",
    "print(len(test) == df.shape[0])\n",
    "\n",
    "for plz in test:\n",
    "    if len(plz) != 5:\n",
    "        print(plz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1016b7d-0706-4c41-9a5a-11d82f50cd73",
   "metadata": {},
   "source": [
    "### Knapp 95% der Einträge haben keine Koordinaten\n",
    "Daher approximieren wir die Koordinaten auf Basis der PLZ mithilfe einer Mapping Tabelle, welche Koordinaten für alle PLZ in Deutschland zur Verfügung stellt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f20ce4-3de1-449b-993f-15c679dd536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nas = df[\"Koordinate: Breitengrad (WGS84)\"].isna().sum()\n",
    "nas / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47abd289-4425-4068-bccc-fd420000a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "nas = df[\"Straße\"].isna().sum()\n",
    "nas / df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fba145",
   "metadata": {},
   "source": [
    "### Approximieren der Koordinaten mit PLZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8e07e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Storage and different NA-Values in data\n",
    "STORAGE_DIR = \"PLZ-Mapping/\"\n",
    "NA_VALUES = [\n",
    "    \"\",\n",
    "    \"#N/A\",\n",
    "    \"#N/A N/A\",\n",
    "    \"#NA\",\n",
    "    \"-1.#IND\",\n",
    "    \"-1.#QNAN\",\n",
    "    \"-NaN\",\n",
    "    \"-nan\",\n",
    "    \"1.#IND\",\n",
    "    \"1.#QNAN\",\n",
    "    \"<NA>\",\n",
    "    \"N/A\",\n",
    "    \"NULL\",\n",
    "    \"NaN\",\n",
    "    \"n/a\",\n",
    "    \"nan\",\n",
    "    \"null\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abedab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Function\n",
    "def get_data(country: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the data from disk; otherwise download and save it\"\"\"\n",
    "\n",
    "    data_path = os.path.join(STORAGE_DIR, country.upper() + \".txt\")\n",
    "    zip_codes_csv = os.path.join(STORAGE_DIR, \"zip_codes.csv\")\n",
    "\n",
    "    if os.path.exists(data_path):\n",
    "        data = pd.read_csv(\n",
    "            data_path,\n",
    "            dtype={\"postal_code\": str},\n",
    "            na_values=NA_VALUES,\n",
    "            keep_default_na=False,\n",
    "        )\n",
    "        print(\"Data_path exists\")\n",
    "\n",
    "    if os.path.exists(zip_codes_csv):\n",
    "        additional_data = pd.read_csv(zip_codes_csv, sep=\";\")\n",
    "        print(\"Additional data loaded from CSV.\")\n",
    "        data['postal_code'] = data['postal_code'].astype(str)\n",
    "        additional_data['postal_code'] = additional_data['postal_code'].astype(str)\n",
    "\n",
    "        print(\"Vorher: \", data.shape[0])\n",
    "\n",
    "        # Use an outer join to make sure all postal codes are included\n",
    "        data = pd.merge(data, additional_data, on='postal_code', how='outer', suffixes=('', '_add'))\n",
    "\n",
    "        # For latitude and longitude, use coordinates from additional_data only where they are missing in data\n",
    "        data['latitude'] = data['latitude'].fillna(data['latitude_add'])\n",
    "        data['longitude'] = data['longitude'].fillna(data['longitude_add'])\n",
    "\n",
    "        # Now drop the additional columns as they are no longer needed\n",
    "        data.drop(['latitude_add', 'longitude_add'], axis=1, inplace=True)\n",
    "\n",
    "        # Fill remaining NaNs in other columns if necessary\n",
    "        data.fillna('nan', inplace=True)\n",
    "\n",
    "        print(\"Additional data added\")  \n",
    "\n",
    "        # Identify and drop duplicate entries in the 'postal_code' column\n",
    "        data.drop_duplicates(subset='postal_code', keep='first', inplace=True)\n",
    "\n",
    "        print(\"We have so much data: \", data.shape[0])  \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10014b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_data = get_data(\"DE\")\n",
    "mapping_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f7bb216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(mapping_data, postcode):\n",
    "    entry = mapping_data[mapping_data['postal_code'] == postcode]\n",
    "    if not entry.empty:\n",
    "        # Get the longitude and latitude\n",
    "        longitude = entry['longitude'].values[0]\n",
    "        latitude = entry['latitude'].values[0]\n",
    "        if pd.notna(longitude) and pd.notna(latitude):\n",
    "            return (latitude, longitude)\n",
    "        else:\n",
    "            print(f\"Coordinates are not available for postcode: {postcode}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Could not find coordinates for postcode: {postcode}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f90b8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(df):\n",
    "    # Vorverarbeitung für fehlende Koordinaten\n",
    "    missing_coords = df['Koordinate: Breitengrad (WGS84)'].isna() | df['Koordinate: Längengrad (WGS84)'].isna()\n",
    "    missing_postcodes = df.loc[missing_coords, 'Postleitzahl']\n",
    "    \n",
    "    # Anzahl der Zeilen, die aktualisiert werden müssen\n",
    "    total_missing = missing_coords.sum()\n",
    "\n",
    "    # Hole Koordinaten für fehlende Postleitzahlen\n",
    "    unique_postcodes = missing_postcodes.dropna().unique()\n",
    "    print(\"Anzahl durchsuchte PLZs: \", len(unique_postcodes))\n",
    "    coords_dict = {pc: get_coordinates(mapping_data, pc) for pc in unique_postcodes if pc}\n",
    "    print(\"Anzahl gefundene Koordinaten: \", len(coords_dict))\n",
    "    \n",
    "\n",
    "    # Aktualisiere die Koordinaten im DataFrame\n",
    "    for index, row in df[missing_coords].iterrows():\n",
    "            \n",
    "        coords = coords_dict.get(row['Postleitzahl'])\n",
    "        if coords:\n",
    "            df.at[index, 'Koordinate: Breitengrad (WGS84)'] = coords[0]\n",
    "            df.at[index, 'Koordinate: Längengrad (WGS84)'] = coords[1]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ef304fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_coordinates(df):\n",
    "    updated_df = process_chunk(df)\n",
    "    \n",
    "    # Drop rows where 'Koordinate: Breitengrad (WGS84)' or 'Koordinate: Längengrad (WGS84)' is NaN\n",
    "    updated_df.dropna(subset=['Koordinate: Breitengrad (WGS84)', 'Koordinate: Längengrad (WGS84)'], inplace=True)\n",
    "    \n",
    "    print(\"Gelöschte Zeilen: \", df.shape[0] - updated_df.shape[0])\n",
    "\n",
    "    if not updated_df.empty:\n",
    "        updated_df.to_csv(\"updated_data.csv\", index=False, sep=\";\")\n",
    "        print(\"Data processed and saved.\")\n",
    "    else:\n",
    "        print(\"No updates for the data.\")\n",
    "\n",
    "    return updated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25598fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_values = df[\"Postleitzahl\"].nunique()\n",
    "print(num_unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = fill_missing_coordinates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b0be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape davor: {df.shape}\")\n",
    "print(f\"Shape nur Einträge mit PLZ: {updated_df.shape}\")\n",
    "print(f\"Anzahl gelöschte Zeilen: {df.shape[0] - updated_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4599c30",
   "metadata": {},
   "source": [
    "### Hinzufügen Landkreis Spalte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "333aaf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GeoJSON \n",
    "with open('nrw-postleitzahlen.geojson', 'r') as f:\n",
    "    landkreis_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e11dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from plz_code to krs_name\n",
    "plz_to_krs = {feature['properties']['plz_code']: feature['properties']['krs_name'] for feature in landkreis_data['features']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6901e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Landkreis column and fill based on mapping table\n",
    "updated_df['Landkreis'] = updated_df['Postleitzahl'].apply(lambda plz: plz_to_krs.get(plz, 'Unbekannt'))\n",
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where 'Landkreis' is None or missing\n",
    "updated_df = updated_df[updated_df['Landkreis'].notna()]\n",
    "updated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f800990",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699c4da-ebf9-4a07-a5b5-963a16bc8b3a",
   "metadata": {},
   "source": [
    "### Umwandlung in .geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "734322c0-86f8-4784-849c-7cb6a0381dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stelle sicher, dass deine Koordinaten als Float vorliegen\n",
    "updated_df['Koordinate: Breitengrad (WGS84)'] = pd.to_numeric(updated_df['Koordinate: Breitengrad (WGS84)'], errors='coerce')\n",
    "updated_df['Koordinate: Längengrad (WGS84)'] = pd.to_numeric(updated_df['Koordinate: Längengrad (WGS84)'], errors='coerce')\n",
    "\n",
    "df_clean = updated_df.drop(columns=['Inbetriebnahmedatum der Einheit formatted'])\n",
    "\n",
    "# Erstelle ein GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df_clean.drop(['Koordinate: Breitengrad (WGS84)', 'Koordinate: Längengrad (WGS84)'], axis=1),\n",
    "    crs=\"EPSG:4326\",\n",
    "    geometry=[Point(xy) for xy in zip(df_clean['Koordinate: Längengrad (WGS84)'], df_clean['Koordinate: Breitengrad (WGS84)'])]\n",
    ")\n",
    "\n",
    "# Konvertiere das GeoDataFrame in GeoJSON\n",
    "geojson = gdf.to_json()\n",
    "\n",
    "# Save iti\n",
    "with open('data_final_mit_Landkreis.geojson', 'w') as f:\n",
    "    f.write(geojson)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compspa_env",
   "language": "python",
   "name": "compspa_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
